version: '3.9'
services:
    postgres:
        image: postgres
        restart: on-failure
        env_file: environment.cfg
        healthcheck:
            test: ["CMD", "pg_isready", "-U", "airflow"]
            interval: 5s
            retries: 5
        restart: always
    redis:
        image: redis:latest
        ports:
            - 6379:6379
        healthcheck:
            test: ["CMD", "redis-cli", "ping"]
            interval: 5s
            timeout: 30s
            retries: 50
        restart: always
    init:
        build: . 
        entrypoint: /bin/bash
        command: 
            - -c 
            - |
                airflow initdb
                gcloud auth activate-service-account --key-file /var/secrets/google/service-account.json
                airflow connections add refinitiv-ftp \
                        --conn-type None \
                        --conn-login $$(gcloud secrets versions access latest --project=fundsaiq --secret=refinitiv_user --format='get(payload.data)' | base64 -d) \
                        --conn-password $$(gcloud secrets versions access latest --project=fundsaiq --secret=refinitiv_pass --format='get(payload.data)' | base64 -d)
                airflow connections add msci-sftp \
                        --conn-type ssh \
                        --conn-host 'sftp.esg.msci.com' \
                        --conn-port '22' \
                        --conn-login $$(gcloud secrets versions access latest --project=fundsaiq --secret=msci_user --format='get(payload.data)' | base64 -d) \
                        --conn-password $$(gcloud secrets versions access latest --project=fundsaiq --secret=msci_pass --format='get(payload.data)' | base64 -d)
                exec /entrypoint db check
        restart: on-failure
        depends_on:
            - postgres
        env_file: environment.cfg
        secrets: 
            - source: creds
              target: /var/secrets/google/service-account.json
              uid: "50000"
              gid: "0"
              mode: 0440
        volumes:
            - ../DAGs:/opt/airflow/dags/DAGs
            - ../scripts:/opt/airflow/dags/scripts
            - ../logs:/opt/airflow/logs
    scheduler:
        build: .
        command: scheduler
        restart: always
        depends_on:
            - init
            - webserver
        env_file: environment.cfg
        secrets: 
            - source: creds
              target: /var/secrets/google/service-account.json
              uid: "50000"
              gid: "0"
              mode: 0440
        healthcheck:
            test: ["CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"']
            interval: 10s
            timeout: 10s
            retries: 5
        volumes:
            - ../DAGs:/opt/airflow/dags/DAGs
            - ../scripts:/opt/airflow/dags/scripts
            - ../logs:/opt/airflow/logs
    webserver:
        build: .
        command: webserver
        restart: on-failure
        depends_on:
            - init
        healthcheck:
            test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
            interval: 10s
            timeout: 10s
            retries: 5
        env_file: environment.cfg
        secrets: 
            - source: creds
              target: /var/secrets/google/service-account.json
              uid: "50000"
              gid: "0"
              mode: 0440
        volumes:
            - ../DAGs:/opt/airflow/dags/DAGs
            - ../scripts:/opt/airflow/dags/scripts
            - ../logs:/opt/airflow/logs
        ports:
            - "8080:8080"
    worker:
        build: .
        command: worker
        deploy:
            replicas: 2
        env_file: environment.cfg
        secrets: 
            - source: creds
              target: /var/secrets/google/service-account.json
              uid: "50000"
              gid: "0"
              mode: 0440
        volumes:
            - ../DAGs:/opt/airflow/dags/DAGs
            - ../scripts:/opt/airflow/dags/scripts
            - ../logs:/opt/airflow/logs
        depends_on:
            redis:
                condition: service_healthy
            postgres:
                condition: service_healthy
        healthcheck:
            test:
                - "CMD-SHELL"
                - 'celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"'
            interval: 10s
            timeout: 10s
            retries: 5
        restart: always
    flower:
        build: .
        env_file: environment.cfg
        depends_on:
            redis:
                condition: service_healthy
            postgres:
                condition: service_healthy
        command: celery flower
        ports:
            - 5555:5555
        healthcheck:
            test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
            interval: 10s
            timeout: 10s
            retries: 5
        restart: always

secrets:
    creds:
      file: ../creds.json